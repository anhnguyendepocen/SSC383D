  \documentclass{mynotes}

%\geometry{showframe}% for debugging purposes -- displays the margins

\newcommand{\E}{\mbox{E}}

\usepackage{amsmath}
%\usepackage[garamond]{mathdesign}
\usepackage{url}

% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title[Chapter 1 $\cdot$ SSC 384D]{Chapter 1: Preliminaries}
%\author[ ]{ }
\date{}  % if the \date{} command is left out, the current date will be used

% The following package makes prettier tables.  We're all about the bling!
\usepackage{booktabs}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

% Provides paragraphs of dummy text
\usepackage{lipsum}

% These commands are used to pretty-print LaTeX commands
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name

\newcommand{\N}{\mbox{N}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\sigmahat}{\hat{\sigma}}
\newcommand{\betahat}{\hat{\beta}}


\begin{document}

\maketitle% this prints the handout title, author, and date

\section{Bayesian inference in simple conjugate families}

We start with a few of the simplest building blocks for complex multivariate statistical models: the beta/binomial, normal, and inverse-gamma conjugate families.

\begin{enumerate}[(A)]

\item Suppose that we take independent observations $x_1, \ldots, x_N$ from a Bernoulli sampling model with unknown probability $w$.  That is, the $x_i$ are the results of flipping a coin with unknown bias.  Suppose that $w$ is given a Beta(a,b) prior distribution:
$$
p(w) = \frac{\Gamma(a+b)}{\Gamma(a) \cdot \Gamma(b)} \ w^{a-1} (1-w)^{b-1} \, ,
$$
where $\Gamma(\cdot)$ denotes the Gamma function.  Derive the posterior distribution $p(w \mid x_1, \ldots, x_N)$.\footnote{I offer two tips here that are quite general.  (1) Your final expression will be cleaner if you reduce the data to a sufficient statistic.  (2) Start off by ignoring normalization constants (that is, factors in the density function that do not depend upon the unknown parameter, and are only there to make the density integrate to 1.)  At the end, re-instate these normalization constants based on the functional form of the density.}

\item The probability density function (PDF) of a gamma random variable, $x \sim \mbox{Ga}(a,b)$, is
$$
p(x) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp(-bx) \, .
$$
Suppose that $x_1 \sim \mbox{Ga}(a_1,1)$ and that $x_2 \sim \mbox{Ga}(a_2,1)$.  Define two new random variables $y_1 = x_1/(x_1 + x_2)$ and $y_2 = x_1 + x_2$.  Find the joint density for $(y_1, y_2)$ using a direct PDF transformation (and its Jacobian).\footnote{Take care that you apply the important change-of-variable formula from basic probability.  See, e.g., Section 1.2 of \url{http://www.stat.umn.edu/geyer/old/5102/n.pdf}, also available under ``references'' as ``geyer.pdf.''}  Use this to characterize the  marginals $p(y_1)$ and $p(y_2)$, and propose a method that exploits this result to simulate beta random variables, assuming you have a source of gamma random variables.

\item Suppose that we take independent observations $x_1, \ldots, x_N$ from a normal sampling model with unknown mean $\theta$ and \textit{known} variance $\sigma^2$: $x_i \sim \mbox{N}(\theta, \sigma^2)$.  Suppose that $\theta$ is given a normal prior distribution with mean $m$ and variance $v$.  Derive the posterior distribution $p(\theta \mid x_1, \ldots, x_N)$.

\item Suppose that we take independent observations $x_1, \ldots, x_N$ from a normal sampling model with \textit{known} mean $\theta$ but \textit{unknown} variance $\sigma^2$.  (This seems even more artificial than the last, but is conceptually important.)  To make this easier, we will re-express things in terms of the precision, or inverse variance $\omega = 1/\sigma^2$:
$$
p(x_i \mid \theta, \omega) = \left( \frac{\omega}{2 \pi} \right)^{1/2} \exp \left\{ -\frac{\omega}{2} (x_i - \theta)^2 \right\} \, .
$$
Suppose that $\omega$ has a gamma prior with parameters $a$ and $b$, implying that $\sigma^2$ has what is called an inverse-gamma prior.\footnote{Written $\sigma^2 \sim \mbox{IG}(a,b)$.}  Derive the posterior distribution $p(\omega \mid x_1, \ldots, x_N)$.  Re-express this as a posterior for $\sigma^2$, the variance.

\item Suppose that, as above, we take independent observations $x_1, \ldots, x_N$ from a normal sampling model with unknown, common mean $\theta$.  This time, however, each observation has its own idiosyncratic (but known) variance: $x_i \sim \mbox{N}(\theta, \sigma_i^2)$.  Suppose that $\theta$ is given a normal prior distribution with mean $m$ and variance $v$.  Derive the posterior distribution $p(\theta \mid x_1, \ldots, x_N)$.  Express the posterior mean in a form that is clearly interpretable as a weighted average of the observations and the prior mean.

\item Suppose that $(x \mid \sigma^2) \sim \N(0, \sigma^2)$, and that $1/ \sigma^2$ has a Gamma(a,b) prior, defined as above.  Show that the marginal distribution of $x$ is Student's $t$.  This is why the $t$ distribution is often referred to as a \textit{scale mixture of normals}.

\end{enumerate}



\section{The multivariate normal distribution}

\subsection{Basics}

We all know the univariate normal distribution, whose long history began with de Moivre's 18th-century work on approximating the (analytically inconvenient) binomial distribution.  This led to the probability density function
$$
p(x) = \frac{1}{\sqrt{2 \pi v}} \exp \left\{ - \frac{(x-m)^2}{2 v} \right\} \, 
$$
for the normal random variable with mean $m$ and variance $v$, written $x \sim \N(m, v)$.

Here's an alternative characterization of the univariate normal distribution in terms of moment-generating functions:\footnote{Laplace transforms to everybody but statisticians.} a random variable $x$ has a normal distribution if and only if $E \left\{ \exp(tx) \right\} = \exp(mt + v t^2 /2)$ for some real $m$ and positive real $v$.  Remember that $E(\cdot)$ denotes the expected value of its argument under the given probability distribution.  We will generalize this definition to the multivariate normal.

\begin{enumerate}[(A)]

\item First, some simple moment identities.  The covariance matrix $\mbox{cov}(x)$ of a vector-valued random variable $x$ is defined as the matrix whose $(i,j)$ entry is the covariance between $x_i$ and $x_j$.  In matrix notation, $\mbox{cov}(x) = E\{ (x - \mu) (x - \mu)^T \}$, where $\mu$ is the mean vector whose $i$th component is $E(x_i)$.  Prove the following: (1) $\mbox{cov}(x) = E(xx^T) - \mu \mu^T$; and (2) $\mbox{cov}(Ax + b) = A \mbox{cov}(x) A^T$ for matrix $A$ and vector $b$.

\item Consider the random vector $z = (z_1, \ldots, z_p)^T$, with each entry having an independent standard normal distribution (that is, mean 0 and variance 1).  Derive the probability density function (PDF) and moment-generating function (MGF) of $z$, expressed in vector notation.\footnote{Remember that the MGF of a vector-valued random variable $x$ is the expected value of the quantity $\exp(t^T x)$, as a function of the vector argument $t$.}   We say that $z$ has a standard multivariate normal distribution.

\item A vector-valued random variable $x = (x_1, \ldots, x_p)^T$ has a \textit{multivariate normal distribution} if and only if every linear combination of its components is univariate normal.  That is, for all vectors $a$ not identically zero, the scalar quantity $z = a^T x$ is normally distributed.  From this definition, prove that $x$ is multivariate normal, written $x \sim \N(\mu, \Sigma)$, if and only if its moment-generating function is of the form $E(\exp \{t^T x\}) = \exp(t^T \mu + t^T \Sigma t)$.  Hint: what are the mean, variance, and moment-generating function of $z$, expressed in terms of moments of $x$?

\item Another basic theorem is that a random vector is multivariate normal if and only if it is an affine transformation of independent univariate normals.  You will first prove the ``if'' statement.  Let $z$ have a standard multivariate normal distribution, and define the random vector $x = L z + \mu$ for some $p \times p$ matrix $L$ of full column rank.\footnote{The full rank restriction turns out to be unnecessary; relaxing it leads to what is called the \textit{singular normal distribution.}}   Prove that $x$ is multivariate normal.  In addition, use the moment identities you proved above to compute the expected value and covariance matrix of $x$.  

\item Now for the ``only if.''  Suppose that $x$ has a multivariate normal distribution.  Prove that $x$ can be written as an affine transformation of standard normal random variables.  (Note: a good way to prove that something can be done is to do it!)  Use this insight to propose an algorithm for simulating multivariate normal random variables with a specified mean and covariance matrix.

\item Use this last result, together with the PDF of a standard multivariate normal, to show that the PDF of a multivariate normal $x \sim \N(\mu, \Sigma)$ takes the form $p(x) = C \exp\{-Q(x-\mu)/2\}$ for some constant $C$ and quadratic form $Q(x-\mu)$.\footnote{A useful fact is that the Jacobian matrix of the linear map $x \rightarrow Ax$ is simply $A$.}

%\item Let $x$ be as above and suppose that $y = A x$, where $A$ is an $n \times p$ matrix of full column rank (i.e.~its columns are linearly independent).  Compute the MGF of $y$.  If $n < p$, what the PDF of $y$?  If $n>p$, why can we not write down the PDF of $y$?\footnote{This doesn't make it an invalid distribution!  It is perfectly valid, and is usually referred to as the singular normal distribution---singular in the sense of a square matrix that lacks an inverse.}

\item Let $x_1 \sim N(\mu_1, \Sigma_1)$ and $x_2 \sim N(\mu_2, \Sigma_2)$, where $x_1$ and $x_2$ are independent of each other.  Let $y = A x_1 + B x_2$ for matrices $A,B$ of full column rank and appropriate dimension.  Note that $x_1$ and $x_2$ need not have the same dimension, as long as $A x_1$ and $B x_2$ do.  Use your previous results to characterize the distribution of $y$.

\end{enumerate}

\subsection{Conditionals and marginals}

Suppose that $x \sim \N(\mu, \Sigma)$ has a multivariate normal distribution.  Let $x_1$ and $x_2$ denote an arbitrary partition of $x$ into two sets of components.  Because we can relabel the components of $x$ without changing their distribution, we can safely assume that $x_1$ comprises the first $k$ elements of $x$, and $x_2$ the last $p-k$.  We will also assume that $\mu$ and $\Sigma$ have been partitioned conformably with $x$:
$$
\mu = (\mu_1, \mu_2)^T \quad \mbox{and} \quad \Sigma =
\left(
\begin{array}{cc}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22} 
\end{array}
\right) \, .
$$
Clearly $\Sigma_{21} = \Sigma_{12}^T$, as $\Sigma$ is a symmetric matrix.


\begin{enumerate}[(A)]

\item Derive the marginal distribution of $x_1$. (Remember your result about affine transformations.)

\item Derive the conditional distribution for $x_1$, given $x_2$, in terms of the partitioned elements of $x$, $\mu$, and $\Sigma$.  There are multiple approaches here.  If you decide to use Bayes' rule, there are several keys to inner peace: work with densities on a log scale, ignore constants that don't affect $x_1$, and remember the cute trick of completing the square from basic algebra.\footnote{In scalar form:
\begin{eqnarray*}
x^2 - 2bx + c &=& x^2 - 2bx + b^2 - b^2 + c \\
&=& (x-b)^2 - b^2 + c \, .
\end{eqnarray*}
}

\item Let $\Omega = \Sigma^{-1}$ be the inverse covariance matrix, or precision matrix, of $x$, and partition $\Omega$ just as you did $\Sigma$:
$$
\Omega =
\left(
\begin{array}{cc}
\Omega_{11} & \Omega_{12} \\
\Omega_{12}^T & \Omega_{22} 
\end{array}
\right) \, .
$$
Using (or deriving!) identities for the inverse of a partitioned matrix, express each block of $\Omega$ in terms of blocks of $\Sigma$.  Explain briefly how one may interpret the conditional distribution you just derived as a linear regression on $x_2$, where the regression matrix can be read off the precision matrix.

\end{enumerate}

\newpage

\section{Multiple regression: three classical principles for inference}

Suppose we observe data that we believe to follow a linear model, where $y_i = x_i^T \beta + \epsilon_i$ for $i = 1, \ldots, n$.
To fix notation: $y_i$ is a scalar response; $x_i$ is a $p$-vector of predictors or features; and the $\epsilon_i$ are errors.  By convention we write vectors as column vectors.  Thus $x_i^T \beta$ will be our typical way of writing the inner product between the vectors $x_i$ and $\beta$.\marginnote[-4pc]{Notice we have no explicit intercept.  For now you can imagine that all the variables have had their sample means subtracted, making an intercept superfluous.  Or you can just assume that the leading entry in every $x_i$ is equal to 1, in which case $\beta_1$ will be an intercept term.}

Consider three classic inferential principles that are widely used to estimate $\beta$, the vector of regression coefficients.  In this context we will let $\hat{\beta}$ denote an estimate of $\beta$---think, it wears a hat because it's masquerading as the true value.\footnote{This metaphor once came back to me in somewhat garbled fashion on an undergraduate's midterm: ``$\hat{\beta}$ wears a hat because he is an impostor.''}
\begin{description}
\item[Least squares:]make the sum of squared errors as small as possible.
$$
\hat{\beta} = \arg \min_{\beta \in \mathcal{R}^p} \left\{  \sum_{i=1}^n (y_i - x_i^T \beta)^2 \right\}  \, .
$$
\item[Maximum likelihood under Gaussianity:] assume that the errors are independent, mean-zero normal random variables with common variance $\sigma^2$.  Choose $\hat{\beta}$ to maximize the likelihood:
$$
\hat{\beta} = \arg \max_{\beta \in \mathcal{R}^p} \left\{ \prod_{i=1}^n p(y_i \mid \beta, \sigma^2) \right\} \, .
$$
Here $p_i(y_i \mid \sigma^2)$ is the conditional probability density function of $y_i$, given the model parameters $\beta$ and $\sigma^2$.  Two side points: 1) Why a product? 2) Why I have I not written $\hat{\beta}$ explicitly as a function of $\sigma^2$, which is unknown and appears on the right-hand side?

\item[Method of moments:] Choose $\hat{\beta}$ so that the sample covariance between the errors and each of the $p$ predictors is exactly zero.  This gives you a system of $p$ equations and $p$ unknowns.
\end{description}

Show that all three of these principles lead to the same estimator.  You will end up tearing your hair out if you try to deal with sums of scalar quantities.  Thus convert everything to matrix-vector notation.\footnote{Remember your basic results on moments of linear combinations of random variables, and the following two identities on derivatives of linear and quadratic forms:
\begin{eqnarray*}
\frac{\partial (a^Tx)}{\partial x} &=& a \\
\frac{\partial (x^T A x)}{\partial x} &=& (A + A^T) x \, .
\end{eqnarray*}
}

\newpage

\section{Quantifying uncertainty: some basic frequentist ideas}

\subsection{In linear regression}

In frequentist inference, inferential uncertainty is usually characterized by the sampling distribution, which expresses how one's estimate is likely to change under repeated sampling.  The idea is simple: unstable estimators shouldn't be trusted, and should therefore come with large error bars.  This should be a familiar concept, but in case it isn't, consult the tutorial on sampling distributions in this chapter's references.

Suppose, as in the previous section, that we observe data from a linear regression model with Gaussian error:
$$
y = X \beta + \epsilon \; , \quad \epsilon \sim \N(0, \sigma^2 I) \, .
$$

\begin{enumerate}[(A)]

\item Derive the sampling distribution of your estimator for $\beta$ from the previous problem.

\item This sampling distribution depends on $\sigma^2$, yet this is unknown.  Suppose that you still wanted to quantify your uncertainty about the individual regression coefficients.  Propose a strategy for calculating standard errors for each $\beta_j$.  Then consult the data set on ozone concentration in Los Angeles, where the goal is to regress daily ozone concentration on a set of other atmospheric variables.  This is available from the R package ``mlbench,'' with my R script ``ozone.R'' giving you a head start on processing things.

Calculate standard errors using your method, and then using the pre-packaged lm function in R.  Note: you may have an essentially correct strategy for calculating standard errors that yields something slightly different from the lm function.  If so, that's OK---can you explain the discrepancy?

\end{enumerate}

\subsection{Bootstrapping}

If you're unfamiliar with the idea of the bootstrap, consult my tutorial on sampling distributions, along with the review paper by Efron and Gong entitled ``A Leisurely Look at the Bootstrap, the Jackknife, and Cross-Validation.''  The basic idea is to simulate the process of repeated sampling from the population by re-sampling from your sample (with replacement).  The ties and duplicates in your ``bootstrapped samples'' will mimic the sampling variability of the true data-generating process.

Let $\hat{\Sigma}$ denote the covariate matrix of the sampling distribution of $\hat{\beta}$.  Write an R function that will estimate $\hat{\Sigma}$ via bootstrapped resampling for a given design matrix $X$ and response vector $y$.  Use it to compute $\hat{\Sigma}$ for the ozone data set, and compare it to the parametric estimate based on normal theory.

\subsection{Propagating uncertainty}

Suppose you have taken data and estimated some parameters $\theta_1, \ldots, \theta_P$ of a multivariate statistical model---for example, the regression model of the previous problem.  Call your estimate $\thetahat = (\thetahat_1, \ldots, \thetahat_P)^T$.  Suppose that you also have an estimate of the covariance matrix of the sampling distribution of $\thetahat$:
$$
\hat{\Sigma} \approx \mbox{cov}( \thetahat) = E \left\{ (\thetahat - \bar\theta) (\thetahat - \bar\theta)^T \right\} \, ,
$$
where the expectation is under the sampling distribution for the data, given the true parameter $\theta$.  Here $\bar\theta$ denotes the mean of the sampling distribution.

If you want to report uncertainty about the $\thetahat_j$'s, you can do so by peeling off the diagonal of the estimated covariance matrix: $\hat{\Sigma}_{jj} = \sigmahat^2_j$ is the square of the ordinary standard error of $\thetahat_j$.  But what if you want to report uncertainty about some function involving multiple components of the estimate $\thetahat$?

\begin{enumerate}[(A)]
\item Start with the trivial case where you want to estimate
$$
f(\theta) = \theta_1 + \theta_2 \, .
$$
Calculate the standard error of $f(\thetahat)$, and generalize this to the case where $f$ is the sum of all $p$ components of $\thetahat$.

\item What now if $f$ is a nonlinear function of the $\thetahat_j$'s?  Propose an approximation for $\mbox{var} \{ f(\thetahat) \}$, where $f$ is any sufficiently smooth function.  (As above, the variance is under the sampling distribution of the data, given the true parameter.)

There are obviously many potential strategies that might work, but here's one you might find fruitful: try a first-order Taylor approximation of $f(\thetahat)$ around the unknown true value $\theta$.  Try to bound the size of the likely error of the approximation, or at least talk generally about what kinds of assumptions or features of $f$ or $p(\thetahat \mid \theta)$ might be relevant.  You should also reflect on some of the potential caveats of this approach.

\end{enumerate}




\end{document}

