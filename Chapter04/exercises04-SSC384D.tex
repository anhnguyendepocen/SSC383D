  \documentclass{mynotes}

%\geometry{showframe}% for debugging purposes -- displays the margins

\newcommand{\E}{\mbox{E}}
\newcommand{\MSE}{\mbox{MSE}}
\newcommand{\var}{\mbox{var}}


\usepackage{amsmath}
%\usepackage[garamond]{mathdesign}
\usepackage{url}

% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title[Exercises 4 $\cdot$ SSC 383D]{Exercises 4: Kernel density estimation}
%\author[ ]{ }
\date{}  % if the \date{} command is left out, the current date will be used

% The following package makes prettier tables.  We're all about the bling!
\usepackage{booktabs}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

% Provides paragraphs of dummy text
\usepackage{lipsum}

% These commands are used to pretty-print LaTeX commands
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name

\newcommand{\N}{\mbox{N}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\sigmahat}{\hat{\sigma}}
\newcommand{\betahat}{\hat{\beta}}


\begin{document}

\maketitle% this prints the handout title, author, and date

\section{Kernel density estimates}

\subsection{Histograms}

You're already familiar with the simplest nonparametric density estimate, which is the histogram!  Suppose, for simplicity's sake, that we're trying to estimate a density of a random variable $X \sim F$ on the unit interval.  Formally speaking, a histogram is a collection of bins $B_j$, $j \in 1, \ldots m$, where
$$
B_j = \left[\frac{j-1}{m}, \frac{j}{m} \right] \, .
$$
Suppose we have a sample $x_1, \ldots, x_n$ from $F$.  Let $h = 1/m$ be the bin width, and let $y_j$ be the number of observations in $B_j$.  The histogram estimator $\hat{f}(x)$ of the density at point $x$ is
$$
\hat{f}(x) = \sum_{j=1}^{m} \frac{\hat{\pi}_j}{h} I(x \in B_j) \, ,
$$
where $\hat{\pi}_k = y_j/n$ is the fraction of observations in bin $B_j$, and $I(A)$ is the indicator function of the event $A$.

Let $x$ and $m$ be fixed. Let $B_j$ be the bin containing $x$.  Show that
$$
\E \{ \hat{f}(x) \} = \pi_j/h \quad \mbox{and} \quad \var\{ \hat{f}(x)\} = \frac{\pi_j (1-\pi_j)}{nh^2} \, ,
$$
where $\pi_j = \int_{B_j} f(u) du$.


%\section{Some frequentist theory on bandwidth selection}
%
%Suppose that the present and future $x$'s are chosen at random from some density $h(x)$, and that the true function $f$ is twice differentiable.  Let $\sigma^2 = \mbox{var}(\epsilon)$.  Prove that, at some particular point $x$,
%$$
%E \left\{ \left[ y - \hat{f}(x) \right]^2 \right\} = \sigma^2 
%  + h^4 (\tau^2)^2 \left\{ \frac{1}{2}  f''(x) + \frac{f'(x) \ h'(x)}{ h(x) } \right\}^2 + \frac{\sigma^2 \eta}{n b h(x)} + o(b^4) + o\left( \frac{1}{nb} \right)
%$$
%for some $\tau^2$ and $\eta$ that you must calculate.\footnote{Remember $O$ and $o$ notation.}
%
%In case the form of the right-hand side is insufficiently suggestive: try a second-order Taylor expansion.  This one is a bit technical.  To get you started, here's the flavor of the argument using a first-order Taylor expansion.  Expand $f(x_i)$ around the point $x$ where you want to predict:
%$$
%y_i = f(x) + (x-x_i) f'(x) + \epsilon_i \, .
%$$
%Now appeal to the fact that $\hat{f}$ is a linear smoother:
%\begin{eqnarray*}
%\hat{f}(x)  &=&  \sum_{i=1}^n w_i y_i \\
%&=& \sum_{i=1}^n w_i \left\{  f(x) + (x-x_i) f'(x) + \epsilon_i \right\} \, ,
%\end{eqnarray*}
%abbreviating the weights as $w_i$.  Remember that the residuals are mean zero and uncorrelated with anything, and that the weights sum to 1.  Use this to calculate the bias and variance.
%
%Once you prove the result, consider some auxiliary questions.
%
%\begin{enumerate}[(A)]
%\item Explain, intuitively, why the $h'(x)$ term enters the picture.
%\item Use this expression to derive how the optimal choice of $h$ behaves in terms of the sample size $n$.
%\item Explain what this choice of $h$ implies for the asymptotic behavior of the excess error in prediction,
%$$
%E \left\{ \left[ y - \hat{f}(x) \right]^2 \right\} - \sigma^2 \ .
%$$
%It converges to something.  What, and how fast in terms of $n$?
%\item What barriers exist to the practical usage of this result?  Above you could ignore pesky constants that didn't grow or shrink with $n$.  Now think about what's involved in them.
%\end{enumerate}
%
%
%\newpage

\end{document}

